[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Power in the USA\n\n\n\n\n\n\nDuckDB\n\n\nOverture Maps\n\n\nIbis\n\n\nDatashader\n\n\nPython\n\n\n\nVisualizing the power lines and power plants across the US\n\n\n\n\n\nNov 25, 2024\n\n\nColin Hill\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nA River’s History Visualized\n\n\n\n\n\n\nVisualization\n\n\nDEM\n\n\nXarray\n\n\nGeoPandas\n\n\nPython\n\n\n\nVisualizing the meanderings of the Snake River in Grand Teton National Park using a relative elevation model\n\n\n\n\n\nNov 3, 2024\n\n\nColin Hill\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 2, 2024\n\n\nColin Hill\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/power_usa/index.html",
    "href": "posts/power_usa/index.html",
    "title": "Power in the USA",
    "section": "",
    "text": "Electricity is supplied to our homes but we rarely know where it’s coming from. To get a better sense of where power is being generated and how it’s supplied to us, I wanted to visualize the power line network and the power plants. For this I’ll use the infrastructure layer from Overture Maps, along with Ibis with the DuckDB backend to filter the parquet files, and finally plotting the data with datashader.\n\nimport ibis\nfrom ibis import _\nimport geopandas as gpd\nimport shapely\nimport datashader as ds\nfrom datashader.transfer_functions import shade, stack, set_background\n%config InlineBackend.figure_formats = ['png']\n# from PIL import Image\n\nRetrieve the boundary for USA and clip to lower 48 states\n\nadmin_0 = gpd.read_file(\"https://github.com/nvkelso/natural-earth-vector/raw/master/geojson/ne_10m_admin_0_countries.geojson\")\npolygon = admin_0.loc[admin_0['ADM0_A3'].eq('USA'), 'geometry'].clip(shapely.box(-125, 24, -65, 49)).iloc[0]\ncrs = admin_0.crs\n\nNow let’s retrieve the power lines and stations from the Overture Maps infrastructure data. We can use Ibis with the DuckDB backend to filter the parquet files.\n\n%%time\n\n# Initialize DuckDB\ncon = ibis.duckdb.connect(extensions=['spatial'])\n\n# Disable progress bar as it causes kernel to crash\ncon.raw_sql(\"SET enable_progress_bar = false\");\n\n# Load Overture Maps parquet data from s3\nt = con.read_parquet(\"s3://overturemaps-us-west-2/release/2024-11-13.0/theme=base/type=infrastructure/*\", table_name=\"infra-usa\")\n\n# Convert polygon for ibis\ngeometry_expr = ibis.literal(polygon)\n\n# Filter to locations within the polygon, for the 'power' subtype\nexpr = (\n    t\n    .rename(infra_class=\"class\")\n    .filter(\n        _.geometry.within(geometry_expr),\n        _.subtype == \"power\",\n        _.infra_class.isin([\"plant\", \"power_line\", \"minor_line\"])\n    )\n    .select([\"names\", \"geometry\", \"infra_class\"])\n)\n\n# Write to local parquet\ncon.to_parquet(expr, \"usa-power.parquet\")\n\nCPU times: user 6min 51s, sys: 6.37 s, total: 6min 57s\nWall time: 2min 33s\n\n\nFor reference, I processed this data on a Macbook pro M1 with a 1Gbps up / 50Mbps down internet connection.\nNow we can move on to visualizing the data. First, we’ll clean up the geometries before plotting:\n\n# Open local parquet with Geopandas (dataset is fairly small so this is performant)\npower = gpd.read_parquet(\"usa-power.parquet\")\n\n\npower['geom_type'] = power['geometry'].geom_type\npower.groupby(['infra_class', 'geom_type'])['geometry'].count()\n\ninfra_class  geom_type   \nminor_line   LineString       93837\n             Point                3\n             Polygon             43\nplant        LineString           2\n             MultiPolygon      1380\n             Point               26\n             Polygon          10588\npower_line   LineString      242367\n             Point                2\n             Polygon           1155\nName: geometry, dtype: int64\n\n\nOne would expect power_line and minor_line to be LineString type, which they almost entirely are. We can simply drop the other geometry types as there are so few. plant is mainly a mix of Polygon and MultiPolygon, which is expected, and for this project we’ll drop the LineString and convert the Points to a Polygon with a simple buffer.\n\n# Drop power lines that are not LineString\npower = power.loc[~(power['infra_class'].isin(['power_line', 'minor_line']) & power['geom_type'].ne('LineString'))]\n\n\n# Drop plant that are LineString\npower = power.loc[~(power['infra_class'].eq('plant') & power['geom_type'].eq('LineString'))]\n\n\n# Convert Points to a circle of radius 100m\nindex = power['infra_class'].eq('plant') & power['geom_type'].eq('Point')\npower.loc[index, 'geometry'] = power.loc[index, 'geometry'].to_crs('EPSG:3857').buffer(100).to_crs(crs)\n\nNow we can plot the data\n\n# Select subsets for power lines and plants\npower_lines = power.loc[power['infra_class'].isin(['power_line', 'minor_line'])]\npower_plants = power.loc[power['infra_class'].eq('plant')]\n\n\n# Set plot limits from bounds of the polygon with a buffer\nxmin, ymin, xmax, ymax = shapely.box(*polygon.bounds).buffer(1).bounds\n\nwidth = 1200\nheight = 600\n\n# Initialize datashader canvas\ncvs = ds.Canvas(plot_width=width, plot_height=height, x_range=(xmin, xmax), y_range=(ymin, ymax))\n\n# Rasterize/aggregate geometries\npower_lines_agg = cvs.line(power_lines, geometry=\"geometry\", line_width=0.1)\npower_plants_agg = cvs.polygons(power_plants, geometry=\"geometry\")\nboundary_agg = cvs.polygons(gpd.GeoDataFrame({'geometry': polygon}, crs=crs, index=[0]), geometry=\"geometry\")\n\n# Stack and render image\nimg = stack(\n    shade(power_lines_agg, cmap=ds.colors.color_lookup['mediumpurple'], alpha=255),\n    shade(power_plants_agg, cmap=ds.colors.color_lookup['red'], alpha=255),\n)\n\nimg = set_background(img, \"black\")\n\n\nimg.to_pil()\n\n\n\n\n\n\n\n\nWe can see that many power plants are (unsurprisingly) located near dense urban areas, especially along the eastern and western coasts."
  },
  {
    "objectID": "posts/relative_elevation_model/index.html",
    "href": "posts/relative_elevation_model/index.html",
    "title": "A River’s History Visualized",
    "section": "",
    "text": "A wonderful aspect of geospatial data is that it’s both beautiful and informative. In this post I look at the migration of the Snake River in Grand Teton National Park, USA over time by creating a relative elevation model (REM). This takes a digital elevation model (DEM) and de-trends the baseline elevation so it follows the water’s surface. By doing so, we can clearly see the migration of the river’s channels and the associated features such as oxbow lakes, meander scars and terraces.\nThis post was inspired by this blog post and this notebook, and makes use of the HyRiver suite of package for data access: PyNHD for river flowlines and Py3DEP for a high resolution DEM.\n\n# Define AOI bounding box\n\n# Snake River, Grand Teton National Park\nbbox = [-110.583466, 43.790715, -110.509222, 43.848585] # WSG84 coordinates, WSEN\n\nFor the first step we will retrieve the river’s path. We can retrieve the river’s flowline (geometry) from USGS data which is easily accessible using the PyNHD package\n\n\nCode\nimport pynhd\nimport py3dep\nimport geopandas as gpd\nimport shapely\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport pyproj\nimport numpy as np\nimport pygeoutils\nimport xarray as xr\nfrom scipy.spatial import KDTree\n%config InlineBackend.figure_formats = ['png']\n\n\n\n# Connect to service \nwater_data = pynhd.WaterData(\"nhdflowline_network\")\n\n# Get flowlines within bounding box\nflowlines = water_data.bybox(bbox)\n\n\n# This returns a GeoDataFrame\nflowlines.head(2)\n\n\n\n\n\n\n\n\ngeometry\ncomid\nfdate\nresolution\ngnis_id\ngnis_name\nlengthkm\nreachcode\nflowdir\nwbareacomi\n...\nqc_12\nvc_12\nqe_12\nve_12\nlakefract\nsurfarea\nrareahload\nrpuid\nvpuid\nenabled\n\n\n\n\n0\nMULTILINESTRING Z ((-110.54139 43.79185 0, -11...\n23123173\n2005-07-21T04:00:00Z\nMedium\n\n\n3.651\n17040101000094\nWith Digitized\n0\n...\n0.132\n0.53390\n0.132\n0.53390\nNaN\nNaN\nNaN\n17a\n17\n1\n\n\n1\nMULTILINESTRING Z ((-110.54022 43.79151 0, -11...\n23123175\n2001-02-07T05:00:00Z\nMedium\n1603229\nSpread Creek\n0.102\n17040101000095\nWith Digitized\n0\n...\n68.385\n1.23836\n68.385\n1.23836\nNaN\nNaN\nNaN\n17a\n17\n1\n\n\n\n\n2 rows × 138 columns\n\n\n\nWe want to keep the main stream only, which in this case we can easily do by filtering on the gnis_name\n\nflowline = flowlines.loc[flowlines['gnis_name'].eq('Snake River')]\n\n\n# Plot flowlines and bounding box\n\n# Create GeoDataframe with bounding box\ngdf_bbox = gpd.GeoDataFrame(data={'geometry': shapely.box(*bbox)}, index=[0], crs='EPSG:4326')\nm = flowlines.explore(color='blue')\nflowline.explore(color='red', m=m)\ngdf_bbox.explore(color=\"black\", style_kwds={\"fillColor\": \"None\"}, m=m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFor the next step, we’ll retrieve a DEM for the region. For this we can use the Py3DEP package which gives us easy access to the USDA 3DEP database\n\n# Check which resolutions are available in our AOI\ndem_resolutions = py3dep.check_3dep_availability(bbox)\ndem_resolutions\n\n{'1m': True,\n '3m': True,\n '5m': False,\n '10m': True,\n '30m': True,\n '60m': False,\n 'topobathy': False}\n\n\nDEM data is available at up to 1m resolution, however we’ll use 3m to save time and computation. We can simply use py3dep.get_dem to download data for our AOI and chosen resolution and return an xarray.DataArray\n\ndem = py3dep.get_dem(bbox, resolution=3, crs=flowline.crs)\n\n\n# Plot DEM with river overlayed\nfig, ax = plt.subplots(figsize=(6, 4))\ndem.plot(ax=ax, robust=True);\nflowline.plot(ax=ax, color=\"r\");\n\n\n\n\n\n\n\n\nTo de-trend the DEM, we need to find the elevation profile along the river’s flowline. While py3dep.elevation_profile provides this functionality, it uses the 10m DEM, and as we already have 3m data we can use it instead with a little extra work.\n\n# First combine all geometries to a single MultiLineString\nline = shapely.union_all(flowline.line_merge())\n\n# Smooth the line to 3m spacing and extract height from the DEM\n\n# Reproject to meters\nproject = pyproj.Transformer.from_crs(flowline.crs, 5070, always_xy=True).transform\nline_5070 = shapely.ops.transform(project, line)\n\n# Smooth line\nspacing = 3 # 3m spacing\nnpts = int(np.ceil(line_5070.length / spacing))\nline_5070_smooth = pygeoutils.smooth_linestring(line_5070, 0.1, npts)\n\n# Reproject back to original crs\nproject = pyproj.Transformer.from_crs(5070, flowline.crs, always_xy=True).transform\nline_smooth = shapely.ops.transform(project, line_5070_smooth)\n\n# Extract elevation from DEM\nxs, ys = line_smooth.xy\nxs, ys = xr.DataArray(xs, dims='distance'), xr.DataArray(ys, dims='distance')\nriver_dem = dem.interp(x=xs, y=ys, method='nearest').dropna(dim='distance')\n\nTo mitigate artifacts arising from the river geometry and DEM not matching (due to natural change over time), we can ensure that the river elevation decreases or stays constant between pixels\n\n# Ensure the elevation is non-increasing\nriver_dem.values = np.minimum.accumulate(river_dem.values)\n\n\nriver_dem.plot(figsize=(4, 2));\n\n\n\n\n\n\n\n\nNow we need to interpolate the height of the river over the entire AOI. We can do this using the inverse distance weighting (IDW) method.\n\n\nCode\ndef idw(da_in: xr.DataArray, da_out: xr.DataArray, k: int = 10, n: float = 1) -&gt; xr.DataArray:\n    \"\"\"\n    Inverse distance weighted interpolation\n    \n    Args:\n        da_in: Input data to interpolate\n        da_out: Output grid\n        k: Number of nearest points to include\n        n: Exponent of the weighting\n\n    Returns:\n        Interpolated data\n    \"\"\"\n    coords = np.column_stack((da_in.x, da_in.y))\n    kdt = KDTree(coords)\n\n    grid = np.dstack(np.meshgrid(da_out.x, da_out.y)).reshape(-1, 2)\n    distances, indices = kdt.query(grid, k=k)\n\n    weights = np.power(np.reciprocal(distances), n)\n    weights = weights / weights.sum(axis=1, keepdims=True)\n    interp = weights * da_in.to_numpy()[indices]\n    interp = interp.sum(axis=1).reshape((da_out.sizes[\"y\"], da_out.sizes[\"x\"]))\n    interp = xr.DataArray(interp, dims=(\"y\", \"x\"), coords={\"x\": da_out.x, \"y\": da_out.y})\n    return interp\n\n\n\nriver_dem_interp = idw(river_dem, dem, k=200, n=0.5)\n\n\nriver_dem_interp.plot(figsize=(6, 4));\n\n\n\n\n\n\n\n\nFinally, we can compute the REM from the river’s elevation profile and the DEM\n\nrem = dem - river_dem_interp\n\n\n# To simplify plotting, set minimum to zero\nrem = rem - rem.min()\n\n\nax = rem.plot.imshow(figsize=(12,8), cmap=\"YlGnBu\", norm=mpl.colors.LogNorm(vmin=1, vmax=10), add_colorbar=False, add_labels=False);\nax.axes.set_aspect('equal');\nax.axes.axis('off');\n\n\n\n\n\n\n\n\nWe can clearly see the complex pattern of different paths the river has taken over the years, and the extent of the floodplains."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome my blog. This will be a space to jot down ideas, experiments, and things I find interesting. I love having a nice visualization to go along with code and analysis, so to kick off I’ll just include this incredible image of the Yukon Delta in Alaska.\n\n\n\nFalse-color image of the Yukon Delta, Alaska, USA (source)\n\n\n\nTopics\nSome major themes will be:\n\nRemote sensing\nEnvironment\nClimate\nWeather\nDemographics\nGeospatial data\n\nAs well as software and tools such as:\n\nPython\nJulia\nMachine Learning / Artificial intelligence\nCloud native data\nGeospatial software\n\n\n\nTech stack\nI’m building this website with Quarto and Pixi."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dr. Colin Hill",
    "section": "",
    "text": "I’m a data scientist and physicist working at the intersection of remote sensing, climate, environment and machine learning.\nI’ve worked across diverse domains ranging from stellar astrophysics to weather forecasts, from climate data to crops, GHG emissions and soil carbon sequestration. I enjoy working on challenging and impactful problems, and a consistent focus has been creating data-driven insights from sensors using physical principles combined with statistical and machine learning approaches."
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Dr. Colin Hill",
    "section": "",
    "text": "I’m a data scientist and physicist working at the intersection of remote sensing, climate, environment and machine learning.\nI’ve worked across diverse domains ranging from stellar astrophysics to weather forecasts, from climate data to crops, GHG emissions and soil carbon sequestration. I enjoy working on challenging and impactful problems, and a consistent focus has been creating data-driven insights from sensors using physical principles combined with statistical and machine learning approaches."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Dr. Colin Hill",
    "section": "Experience",
    "text": "Experience\nRegrow Ag | 2022-2024\nSenior Data Scientist\nTechnical Lead\nWeatherForce | 2019-2021\nSenior Data Scientist\nInstitut de Recherche en Astrophysique et Planétologie | 2016-2019\nPostdoctoral Research Scientist (Astrophysics)"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Dr. Colin Hill",
    "section": "Education",
    "text": "Education\nPh.D. Astrophysics | 2012-2015\nQueen’s University Belfast, UK\nM.Sci. Physics | 2008-2012\nQueen’s University Belfast, UK"
  }
]