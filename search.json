[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Migration of Barn Swallows\n\n\n\n\n\n\nVisualization\n\n\nVector\n\n\nGeoPandas\n\n\nPython\n\n\n\nVisualizing the migration of Barn Swallows across the globe\n\n\n\n\n\nDec 22, 2024\n\n\nColin Hill\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nPower in the USA\n\n\n\n\n\n\nDuckDB\n\n\nOverture Maps\n\n\nIbis\n\n\nDatashader\n\n\nPython\n\n\n\nVisualizing the power lines and power plants across the US\n\n\n\n\n\nNov 25, 2024\n\n\nColin Hill\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nA River’s History Visualized\n\n\n\n\n\n\nVisualization\n\n\nDEM\n\n\nXarray\n\n\nGeoPandas\n\n\nPython\n\n\n\nVisualizing the meanderings of the Snake River in Grand Teton National Park using a relative elevation model\n\n\n\n\n\nNov 3, 2024\n\n\nColin Hill\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 2, 2024\n\n\nColin Hill\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/power_usa/index.html",
    "href": "posts/power_usa/index.html",
    "title": "Power in the USA",
    "section": "",
    "text": "Electricity is supplied to our homes but we rarely know where it’s coming from. To get a better sense of where power is being generated and how it’s supplied to us, I wanted to visualize the power line network and the power plants. For this I’ll use the infrastructure layer from Overture Maps, along with Ibis with the DuckDB backend to filter the parquet files, and finally plotting the data with datashader.\n\nimport ibis\nfrom ibis import _\nimport geopandas as gpd\nimport shapely\nimport datashader as ds\nfrom datashader.transfer_functions import shade, stack, set_background\n%config InlineBackend.figure_formats = ['png']\n# from PIL import Image\n\nRetrieve the boundary for USA and clip to lower 48 states\n\nadmin_0 = gpd.read_file(\"https://github.com/nvkelso/natural-earth-vector/raw/master/geojson/ne_10m_admin_0_countries.geojson\")\npolygon = admin_0.loc[admin_0['ADM0_A3'].eq('USA'), 'geometry'].clip(shapely.box(-125, 24, -65, 49)).iloc[0]\ncrs = admin_0.crs\n\nNow let’s retrieve the power lines and stations from the Overture Maps infrastructure data. We can use Ibis with the DuckDB backend to filter the parquet files.\n\n%%time\n\n# Initialize DuckDB\ncon = ibis.duckdb.connect(extensions=['spatial'])\n\n# Disable progress bar as it causes kernel to crash\ncon.raw_sql(\"SET enable_progress_bar = false\");\n\n# Load Overture Maps parquet data from s3\nt = con.read_parquet(\"s3://overturemaps-us-west-2/release/2024-11-13.0/theme=base/type=infrastructure/*\", table_name=\"infra-usa\")\n\n# Convert polygon for ibis\ngeometry_expr = ibis.literal(polygon)\n\n# Filter to locations within the polygon, for the 'power' subtype\nexpr = (\n    t\n    .rename(infra_class=\"class\")\n    .filter(\n        _.geometry.within(geometry_expr),\n        _.subtype == \"power\",\n        _.infra_class.isin([\"plant\", \"power_line\", \"minor_line\"])\n    )\n    .select([\"names\", \"geometry\", \"infra_class\"])\n)\n\n# Write to local parquet\ncon.to_parquet(expr, \"usa-power.parquet\")\n\nCPU times: user 6min 51s, sys: 6.37 s, total: 6min 57s\nWall time: 2min 33s\n\n\nFor reference, I processed this data on a Macbook pro M1 with a 1Gbps up / 50Mbps down internet connection.\nNow we can move on to visualizing the data. First, we’ll clean up the geometries before plotting:\n\n# Open local parquet with Geopandas (dataset is fairly small so this is performant)\npower = gpd.read_parquet(\"usa-power.parquet\")\n\n\npower['geom_type'] = power['geometry'].geom_type\npower.groupby(['infra_class', 'geom_type'])['geometry'].count()\n\ninfra_class  geom_type   \nminor_line   LineString       93837\n             Point                3\n             Polygon             43\nplant        LineString           2\n             MultiPolygon      1380\n             Point               26\n             Polygon          10588\npower_line   LineString      242367\n             Point                2\n             Polygon           1155\nName: geometry, dtype: int64\n\n\nOne would expect power_line and minor_line to be LineString type, which they almost entirely are. We can simply drop the other geometry types as there are so few. plant is mainly a mix of Polygon and MultiPolygon, which is expected, and for this project we’ll drop the LineString and convert the Points to a Polygon with a simple buffer.\n\n# Drop power lines that are not LineString\npower = power.loc[~(power['infra_class'].isin(['power_line', 'minor_line']) & power['geom_type'].ne('LineString'))]\n\n\n# Drop plant that are LineString\npower = power.loc[~(power['infra_class'].eq('plant') & power['geom_type'].eq('LineString'))]\n\n\n# Convert Points to a circle of radius 100m\nindex = power['infra_class'].eq('plant') & power['geom_type'].eq('Point')\npower.loc[index, 'geometry'] = power.loc[index, 'geometry'].to_crs('EPSG:3857').buffer(100).to_crs(crs)\n\nNow we can plot the data\n\n# Select subsets for power lines and plants\npower_lines = power.loc[power['infra_class'].isin(['power_line', 'minor_line'])]\npower_plants = power.loc[power['infra_class'].eq('plant')]\n\n\n# Set plot limits from bounds of the polygon with a buffer\nxmin, ymin, xmax, ymax = shapely.box(*polygon.bounds).buffer(1).bounds\n\nwidth = 1200\nheight = 600\n\n# Initialize datashader canvas\ncvs = ds.Canvas(plot_width=width, plot_height=height, x_range=(xmin, xmax), y_range=(ymin, ymax))\n\n# Rasterize/aggregate geometries\npower_lines_agg = cvs.line(power_lines, geometry=\"geometry\", line_width=0.1)\npower_plants_agg = cvs.polygons(power_plants, geometry=\"geometry\")\nboundary_agg = cvs.polygons(gpd.GeoDataFrame({'geometry': polygon}, crs=crs, index=[0]), geometry=\"geometry\")\n\n# Stack and render image\nimg = stack(\n    shade(power_lines_agg, cmap=ds.colors.color_lookup['mediumpurple'], alpha=255),\n    shade(power_plants_agg, cmap=ds.colors.color_lookup['red'], alpha=255),\n)\n\nimg = set_background(img, \"black\")\n\n\nimg.to_pil()\n\n\n\n\n\n\n\n\nWe can see that many power plants are (unsurprisingly) located near dense urban areas, especially along the eastern and western coasts."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome my blog. This will be a space to jot down ideas, experiments, and things I find interesting. I love having a nice visualization to go along with code and analysis, so to kick off I’ll just include this incredible image of the Yukon Delta in Alaska.\n\n\n\nFalse-color image of the Yukon Delta, Alaska, USA (source)\n\n\n\nTopics\nSome major themes will be:\n\nRemote sensing\nEnvironment\nClimate\nWeather\nDemographics\nGeospatial data\n\nAs well as software and tools such as:\n\nPython\nJulia\nMachine Learning / Artificial intelligence\nCloud native data\nGeospatial software\n\n\n\nTech stack\nI’m building this website with Quarto and Pixi."
  },
  {
    "objectID": "posts/relative_elevation_model/index.html",
    "href": "posts/relative_elevation_model/index.html",
    "title": "A River’s History Visualized",
    "section": "",
    "text": "A wonderful aspect of geospatial data is that it’s both beautiful and informative. In this post I look at the migration of the Snake River in Grand Teton National Park, USA over time by creating a relative elevation model (REM). This takes a digital elevation model (DEM) and de-trends the baseline elevation so it follows the water’s surface. By doing so, we can clearly see the migration of the river’s channels and the associated features such as oxbow lakes, meander scars and terraces.\nThis post was inspired by this blog post and this notebook, and makes use of the HyRiver suite of package for data access: PyNHD for river flowlines and Py3DEP for a high resolution DEM.\n\n# Define AOI bounding box\n\n# Snake River, Grand Teton National Park\nbbox = [-110.583466, 43.790715, -110.509222, 43.848585] # WSG84 coordinates, WSEN\n\nFor the first step we will retrieve the river’s path. We can retrieve the river’s flowline (geometry) from USGS data which is easily accessible using the PyNHD package\n\n\nCode\nimport pynhd\nimport py3dep\nimport geopandas as gpd\nimport shapely\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport pyproj\nimport numpy as np\nimport pygeoutils\nimport xarray as xr\nfrom scipy.spatial import KDTree\n%config InlineBackend.figure_formats = ['png']\n\n\n\n# Connect to service \nwater_data = pynhd.WaterData(\"nhdflowline_network\")\n\n# Get flowlines within bounding box\nflowlines = water_data.bybox(bbox)\n\n\n# This returns a GeoDataFrame\nflowlines.head(2)\n\n\n\n\n\n\n\n\ngeometry\ncomid\nfdate\nresolution\ngnis_id\ngnis_name\nlengthkm\nreachcode\nflowdir\nwbareacomi\n...\nqc_12\nvc_12\nqe_12\nve_12\nlakefract\nsurfarea\nrareahload\nrpuid\nvpuid\nenabled\n\n\n\n\n0\nMULTILINESTRING Z ((-110.54139 43.79185 0, -11...\n23123173\n2005-07-21T04:00:00Z\nMedium\n\n\n3.651\n17040101000094\nWith Digitized\n0\n...\n0.132\n0.53390\n0.132\n0.53390\nNaN\nNaN\nNaN\n17a\n17\n1\n\n\n1\nMULTILINESTRING Z ((-110.54022 43.79151 0, -11...\n23123175\n2001-02-07T05:00:00Z\nMedium\n1603229\nSpread Creek\n0.102\n17040101000095\nWith Digitized\n0\n...\n68.385\n1.23836\n68.385\n1.23836\nNaN\nNaN\nNaN\n17a\n17\n1\n\n\n\n\n2 rows × 138 columns\n\n\n\nWe want to keep the main stream only, which in this case we can easily do by filtering on the gnis_name\n\nflowline = flowlines.loc[flowlines['gnis_name'].eq('Snake River')]\n\n\n# Plot flowlines and bounding box\n\n# Create GeoDataframe with bounding box\ngdf_bbox = gpd.GeoDataFrame(data={'geometry': shapely.box(*bbox)}, index=[0], crs='EPSG:4326')\nm = flowlines.explore(color='blue')\nflowline.explore(color='red', m=m)\ngdf_bbox.explore(color=\"black\", style_kwds={\"fillColor\": \"None\"}, m=m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFor the next step, we’ll retrieve a DEM for the region. For this we can use the Py3DEP package which gives us easy access to the USDA 3DEP database\n\n# Check which resolutions are available in our AOI\ndem_resolutions = py3dep.check_3dep_availability(bbox)\ndem_resolutions\n\n{'1m': True,\n '3m': True,\n '5m': False,\n '10m': True,\n '30m': True,\n '60m': False,\n 'topobathy': False}\n\n\nDEM data is available at up to 1m resolution, however we’ll use 3m to save time and computation. We can simply use py3dep.get_dem to download data for our AOI and chosen resolution and return an xarray.DataArray\n\ndem = py3dep.get_dem(bbox, resolution=3, crs=flowline.crs)\n\n\n# Plot DEM with river overlayed\nfig, ax = plt.subplots(figsize=(6, 4))\ndem.plot(ax=ax, robust=True);\nflowline.plot(ax=ax, color=\"r\");\n\n\n\n\n\n\n\n\nTo de-trend the DEM, we need to find the elevation profile along the river’s flowline. While py3dep.elevation_profile provides this functionality, it uses the 10m DEM, and as we already have 3m data we can use it instead with a little extra work.\n\n# First combine all geometries to a single MultiLineString\nline = shapely.union_all(flowline.line_merge())\n\n# Smooth the line to 3m spacing and extract height from the DEM\n\n# Reproject to meters\nproject = pyproj.Transformer.from_crs(flowline.crs, 5070, always_xy=True).transform\nline_5070 = shapely.ops.transform(project, line)\n\n# Smooth line\nspacing = 3 # 3m spacing\nnpts = int(np.ceil(line_5070.length / spacing))\nline_5070_smooth = pygeoutils.smooth_linestring(line_5070, 0.1, npts)\n\n# Reproject back to original crs\nproject = pyproj.Transformer.from_crs(5070, flowline.crs, always_xy=True).transform\nline_smooth = shapely.ops.transform(project, line_5070_smooth)\n\n# Extract elevation from DEM\nxs, ys = line_smooth.xy\nxs, ys = xr.DataArray(xs, dims='distance'), xr.DataArray(ys, dims='distance')\nriver_dem = dem.interp(x=xs, y=ys, method='nearest').dropna(dim='distance')\n\nTo mitigate artifacts arising from the river geometry and DEM not matching (due to natural change over time), we can ensure that the river elevation decreases or stays constant between pixels\n\n# Ensure the elevation is non-increasing\nriver_dem.values = np.minimum.accumulate(river_dem.values)\n\n\nriver_dem.plot(figsize=(4, 2));\n\n\n\n\n\n\n\n\nNow we need to interpolate the height of the river over the entire AOI. We can do this using the inverse distance weighting (IDW) method.\n\n\nCode\ndef idw(da_in: xr.DataArray, da_out: xr.DataArray, k: int = 10, n: float = 1) -&gt; xr.DataArray:\n    \"\"\"\n    Inverse distance weighted interpolation\n    \n    Args:\n        da_in: Input data to interpolate\n        da_out: Output grid\n        k: Number of nearest points to include\n        n: Exponent of the weighting\n\n    Returns:\n        Interpolated data\n    \"\"\"\n    coords = np.column_stack((da_in.x, da_in.y))\n    kdt = KDTree(coords)\n\n    grid = np.dstack(np.meshgrid(da_out.x, da_out.y)).reshape(-1, 2)\n    distances, indices = kdt.query(grid, k=k)\n\n    weights = np.power(np.reciprocal(distances), n)\n    weights = weights / weights.sum(axis=1, keepdims=True)\n    interp = weights * da_in.to_numpy()[indices]\n    interp = interp.sum(axis=1).reshape((da_out.sizes[\"y\"], da_out.sizes[\"x\"]))\n    interp = xr.DataArray(interp, dims=(\"y\", \"x\"), coords={\"x\": da_out.x, \"y\": da_out.y})\n    return interp\n\n\n\nriver_dem_interp = idw(river_dem, dem, k=200, n=0.5)\n\n\nriver_dem_interp.plot(figsize=(6, 4));\n\n\n\n\n\n\n\n\nFinally, we can compute the REM from the river’s elevation profile and the DEM\n\nrem = dem - river_dem_interp\n\n\n# To simplify plotting, set minimum to zero\nrem = rem - rem.min()\n\n\nax = rem.plot.imshow(figsize=(12,8), cmap=\"YlGnBu\", norm=mpl.colors.LogNorm(vmin=1, vmax=10), add_colorbar=False, add_labels=False);\nax.axes.set_aspect('equal');\nax.axes.axis('off');\n\n\n\n\n\n\n\n\nWe can clearly see the complex pattern of different paths the river has taken over the years, and the extent of the floodplains."
  },
  {
    "objectID": "posts/bird_migration/index.html",
    "href": "posts/bird_migration/index.html",
    "title": "Migration of Barn Swallows",
    "section": "",
    "text": "Barn Swallow\n\n\nBarn Swallows are the most widespread species of swallow in the world, occurring on all continents. They prefer habitats in the open countryside with low vegetation such as pasture, meadows and farmland, with nearby water. The majority of barn swallows breed across the Northern Hemisphere in the summer, migrating to southern Africa and South America for the winter. In this post we’ll visualize the migration of Barn Swallows across the globe using Python, GeoPandas and MovingPandas, with data sourced from Movebank.\n\nimport pandas as pd\nimport geopandas as gpd\nimport movingpandas as mpd\n\nThe first step is to download the relevant datasets from Movebank. We’ll use data from two studies to cover parts of Europe/Africa and North/South America, namely:\n\nStudy 1\n\nBarn swallows breeding in Kraghede\nData spans 2016-2017, where the northward migration is in 2017\n4 individuals were tracked\n\nStudy 2\n\nCLSW_Nebraska_2022\nData spans 2021-2022, where the northward migration is in 2022\n1 individual was tracked\n\n\nThe data can be downloaded as CSV files, which we’ll use below\n\nstudy_1 = pd.read_csv('Barn swallows breeding in Kraghede.csv')\nstudy_2 = pd.read_csv('CLSW_Nebraska_2022.csv')\n\ndef preprocess(df):\n    # Drop missing coordinates\n    df.dropna(subset=['location-lat', 'location-long'], inplace=True)\n    \n    # Convert timstamp to datetime\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n\n    # Convert to GeoDataFrame\n    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['location-long'], df['location-lat']), crs='EPSG:4326')\n\n    return gdf\n\nstudy_1 = preprocess(study_1)\nstudy_2 = preprocess(study_2)\n\n# Add migration direction\nstudy_1.loc[study_1['timestamp'].dt.year.eq(2016), 'd'] = 'southward'\nstudy_1.loc[study_1['timestamp'].dt.year.eq(2017), 'd'] = 'northward'\nstudy_2.loc[study_2['timestamp'].dt.year.eq(2021), 'd'] = 'southward'\nstudy_2.loc[study_2['timestamp'].dt.year.eq(2022), 'd'] = 'northward'\ngdf = pd.concat([study_1, study_2])\n\nIndividual birds are identified by the ‘individual-local-identifier’ column. The dataframes have timestamps for each observation, with point coordinates in the ‘location-lat’ and ‘location-long’ columns. Let’s quickly check how many observations we have for each individual, per year\n\ngdf.groupby(['study-name', 'individual-local-identifier', 'd']).size()\n\nstudy-name                          individual-local-identifier  d        \nBarn swallows breeding in Kraghede  9AD3712                      northward    232\n                                                                 southward    220\n                                    9AK1947                      northward    257\n                                                                 southward    185\n                                    9AK1973                      northward    310\n                                                                 southward    219\n                                    9AP0465                      northward    230\n                                                                 southward    191\nCLSW_Nebraska_2022                  2401-22580                   northward    251\n                                                                 southward    310\ndtype: int64\n\n\nTo plot the migration paths, we’ll need to create LineStrings from the point coordinates, which we can easily do with MovingPandas. We’ll split the data into two separate dataframes, one for the southward migration and one for the northward migration\n\ngdf_southward = gdf.loc[gdf['d'].eq('southward')]\ngdf_northward = gdf.loc[gdf['d'].eq('northward')]\n\nNow we can generate the trajectories\n\nsouthward_traj = mpd.TrajectoryCollection(gdf_southward, 'individual-local-identifier', t='timestamp')\nnorthward_traj = mpd.TrajectoryCollection(gdf_northward, 'individual-local-identifier', t='timestamp')\n\nWith the trajectories, we can see the time taken and distance travelled by the birds in each direction\n\n# The study in which individual birds belong\nindividual_study_map = gdf.groupby('individual-local-identifier')['study-name'].first().to_dict()\n\ntotal = []\nfor direction, traj,  in zip(['southward', 'northward'], [southward_traj, northward_traj]):\n    _gdf = traj.to_traj_gdf()\n    _gdf['direction'] = direction\n    _gdf['duration'] = _gdf['end_t'] - _gdf['start_t']\n    _gdf['km'] = _gdf['length'] / 1_000\n    _gdf['km/day'] = _gdf['km'] / _gdf['duration'].dt.days\n    total.append(_gdf)\ntotal = pd.concat(total)\ntotal['study'] = total['individual-local-identifier'].map(individual_study_map)\n\n\ntotal[['duration', 'km', 'km/day']].max(axis=0)\n\nduration    158 days 22:49:55\nkm               23144.292571\nkm/day             178.636679\ndtype: object\n\n\nThe barn swallows in these studies flew up to ~23,100 km in ~158 days, an average of ~179 km/day\n\ntotal.groupby(['study', 'direction'])[['duration', 'km', 'km/day']].mean().round(1)\n\n\n\n\n\n\n\n\n\nduration\nkm\nkm/day\n\n\nstudy\ndirection\n\n\n\n\n\n\n\nBarn swallows breeding in Kraghede\nnorthward\n137 days 00:45:49\n14688.1\n106.6\n\n\nsouthward\n108 days 21:29:45.750000\n10282.7\n94.9\n\n\nCLSW_Nebraska_2022\nnorthward\n125 days 03:06:10\n22329.6\n178.6\n\n\nsouthward\n154 days 08:41:58\n18345.9\n119.1\n\n\n\n\n\n\n\nOn average, the birds flying from Northern Europe to Africa took less time to migrate South, but longer to migrate North compared to the one example from North America to South America. The average distance covered was also much lower for the Europe/Africa migration.\nWe can visualize these trajectories using the MovingPandas TrajectoryCollection objects. On the map the starting points are shown as triangles, where the solid lines represent the southward migration and the dashed lines represent the northward migration\n\nparams = dict(line_width=3, geo=True, tiles=\"EsriImagery\", width=760, height=600)\nsouthward_traj.hvplot(**params) * northward_traj.hvplot(**params, line_dash='dashed')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dr. Colin Hill",
    "section": "",
    "text": "I’m a data scientist and physicist working at the intersection of remote sensing, climate, environment and machine learning.\nI’ve worked across diverse domains ranging from stellar astrophysics to weather forecasts, from climate data to crops, GHG emissions and soil carbon sequestration. I enjoy working on challenging and impactful problems, and a consistent focus has been creating data-driven insights from sensors using physical principles combined with statistical and machine learning approaches."
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Dr. Colin Hill",
    "section": "",
    "text": "I’m a data scientist and physicist working at the intersection of remote sensing, climate, environment and machine learning.\nI’ve worked across diverse domains ranging from stellar astrophysics to weather forecasts, from climate data to crops, GHG emissions and soil carbon sequestration. I enjoy working on challenging and impactful problems, and a consistent focus has been creating data-driven insights from sensors using physical principles combined with statistical and machine learning approaches."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Dr. Colin Hill",
    "section": "Experience",
    "text": "Experience\nRegrow Ag | 2022-2024\nSenior Data Scientist\nTechnical Lead\nWeatherForce | 2019-2021\nSenior Data Scientist\nInstitut de Recherche en Astrophysique et Planétologie | 2016-2019\nPostdoctoral Research Scientist (Astrophysics)"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Dr. Colin Hill",
    "section": "Education",
    "text": "Education\nPh.D. Astrophysics | 2012-2015\nQueen’s University Belfast, UK\nM.Sci. Physics | 2008-2012\nQueen’s University Belfast, UK"
  }
]